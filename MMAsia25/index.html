<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HD-Vision@MMAsia2025</title>
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
  <!-- Hero Section -->
  <section class="hero">
    <h1>International Workshop on Imaging, Processing, Perception, and Reasoning for High-Dimensional Visual Data</h1>
    <p class="subtitle">ACM Multimedia Asia 2025 Workshop</p>
    <p class="details">
      <i class="fa fa-calendar"></i> December 9 – 12, 2025 &nbsp;
      <i class="fa fa-map-marker"></i> Grand Millenium Hotel, Kuala Lumpur, Malaysia
    </p>
  </section>

  <!-- Main Content Container -->
  <div class="container">
    <!-- Summary Section -->
    <section id="summary">
      <h2>Summary</h2>
        <p>
          The International Workshop on Imaging, Processing, Perception, and Reasoning for High-Dimensional Visual Data (HD-Vision) addresses the rapidly evolving frontier of multimedia research, where visual information extends far beyond conventional 2D imagery. Emerging modalities such as light fields, event-based data, hyperspectral imaging, and multimodal sensor fusion encode rich spatial, temporal, angular, spectral, and cross-modal cues, unlocking unprecedented opportunities for comprehensive scene understanding.
        </p>
        <p>
          These modalities also introduce fundamental challenges in sensing, representation, and interpretation: the demand for novel acquisition techniques, efficient compression and transmission, robust neural reconstruction, and semantics-aware reasoning. HD-Vision aims to unite researchers from computational imaging, multimodal learning, and neural representation fields to confront these challenges and bridge the gap between foundational theory and practical deployment.
        </p>
        <p>
          The workshop serves as a collaborative platform to present state-of-the-art advances and visionary perspectives, fostering interdisciplinary solutions applicable to domains such as autonomous systems, AR/VR, intelligent robotics, medical diagnostics, and remote sensing. By integrating diverse expertise, HD-Vision seeks to catalyze the next generation of high-dimensional multimedia understanding.
        </p>
    </section>


    <!-- Call for Papers Section -->
    <section id="Call For Paper">
      <h2>Call for Papers</h2>
      <p>We invite original submissions that address challenges and advances across the full spectrum of high-dimensional multimedia understanding. Topics of interest include, but are not limited to:</p>

      <ul>
        <li><b>High-Dimensional Visual Sensing and Computational Imaging</b><br>
          Techniques for capturing complex modalities such as light fields, event-based data, hyperspectral imaging, and multimodal sensor systems.
        </li>
        <li><b>Compression and Neural Representations for Complex Modalities</b><br>
          Efficient encoding, representation, and transmission methods, including learning-based codecs, neural implicit representations, NeRFs, and Gaussian splatting.
        </li>
        <li><b>Semantic Understanding and Cross-Modal Perception</b><br>
          High-level interpretation of spatial, temporal, and angular information, including multi-modal data registration, fusion, and semantic parsing.
        </li>
        <li><b>Vision-Language Reasoning for Multi-modal Data</b><br>
          Foundation models and LLMs for spatially grounded multimodal reasoning, including pretraining strategies, parameter-efficient adaptation, and cross-modal alignment.
        </li>
        <li><b>Datasets and Benchmarks for High-Dimensional Media</b><br>
          Construction, annotation, and evaluation of datasets spanning novel sensing modalities and complex data distributions.
        </li>
        <li><b>Trustworthy and Efficient Multi-modal Intelligence</b><br>
          Energy-aware, robust, and privacy-preserving systems for high-dimensional data processing, including ethical concerns and deployment efficiency on edge devices.
        </li>
      </ul>

        <!-- Submission Links -->
      <p><b>Submission Website:</b> 
        <a href="https://cmt3.research.microsoft.com/MMAsia2025" target="_blank">
          Submit via CMT
        </a>
      </p>
      <p><b>Download CFP (PDF):</b> 
        <a href="assets/pdfs/CFP.pdf" target="_blank">
          Click here to download
        </a>
      </p>
      <p><b>Important Registration Note:</b>  
        All accepted papers need to be covered by a 
        <span style="color: red; font-weight: bold;">full registration</span>.  
        <a href="https://mmasia2025.org/" target="_blank">Click here to register</a>.
      </p>
    </section>

    <!-- Speakers Section -->
    <section id="speakers">
      <h2>Keynote Speaker</h2>
      <div class="speaker-grid">
        <div class="speaker">
          <img src="assets/imgs/avatar_speaker_yiping.jpg" alt="Yi-Ping">
          <h3>Prof. Yi-Ping Phoebe Chen</h3>
          <p>(La Trobe University, Australia)</p>
        </div>
      </div>
    </section>


    <!-- Speakers Section -->
    <section id="speakers">
      <h2>Speakers</h2>
      <div class="speaker-grid">
        <div class="speaker">
          <img src="assets/imgs/avatar_organizer_4.png" alt="Shiyu Xuan">
          <h3>Dr. Cong Zhang</h3>
          <p>(CUHK, HKSAR)</p>
        </div>
        <div class="speaker">
          <img src="assets/imgs/avatar_speaker_hanyuzhou.jpg" alt="Hanyu Zhou">
          <h3>Dr. Hanyu Zhou</h3>
          <p>(NUS, Singapore)</p>
        </div>
        <div class="speaker">
          <img src="assets/imgs/avatar_speaker_zixiangzhao.jpg" alt="Zixiang Zhao">
          <h3>Dr. Zixiang Zhao</h3>
          <p>(ETH Zürich, Switzerland)</p>
        </div>
      </div>
    </section>

    <!-- Schedule Section -->
  <section id="schedule">
    <h2>Schedule</h2>
    <p><b><em>Note: The schedule is for reference only and is subject to change.</em></b></p>

    <table>
      <tr>
        <th>Time</th>
        <th>Event</th>
      </tr>

      <!-- Opening -->
      <tr>
        <td>14:00 - 14:05</td>
        <td>Welcome &amp; Session Introduction</td>
      </tr>

      <!-- Oral Session: 4 talks, each 8 min (+2 for Q&A inside block) -->
      <tr>
        <td>14:05 - 14:45</td>
        <td>Oral Session (4 contributed talks, 8+2 min each)</td>
      </tr>

      <!-- Keynote: Phoebe Chen -->
      <tr>
        <td>14:45 - 15:25</td>
        <td>Keynote: Prof. Yi-Ping Phoebe Chen (40 min)</td>
      </tr>

      <!-- Coffee Break + Posters -->
      <tr>
        <td>15:30 - 16:00</td>
        <td>Coffee Break</td>
      </tr>

      
      <!-- Invited Speaker Talk #1 -->
      <tr>
        <td>16:00 - 16:20</td>
        <td>Invited Speaker Talk #1: Dr. Cong Zhang (20 min)</td>
      </tr>


      <!-- Invited Speaker Talk #2 -->
      <tr>
        <td>16:20 - 16:40</td>
        <td>Invited Speaker Talk #2: Dr. Hanyu Zhou (20 min)</td>
      </tr>

      <!-- Invited Speaker Talk #3 -->
      <tr>
        <td>16:40 - 17:00</td>
        <td>Invited Speaker Talk #3: Dr. Zixiang Zhao (20 min)</td>
      </tr>

      <!-- Awards & Closing -->
      <tr>
        <td>17:00 - 17:10</td>
        <td>Awards &amp; Closing (Best Paper, etc.)</td>
      </tr>
    </table>
</section>


  <!-- Accepted Papers Section -->
<section id="accepted-papers">
  <h2>Accepted Papers</h2>

  <table class="paper-table">
    <tr>
      <th>Title</th>
      <th>Type</th>
    </tr>
    <tr>
      <td>Exploiting Appearance Re-Emergence for Robust Visual Tracking</td>
      <td>Oral</td>
    </tr>
    <tr>
      <td>Spike Camera Image Reconstruction Based on an Efficient Spiking Transformer</td>
      <td>Oral</td>
    </tr>
    <tr>
      <td>PanoExtend: An Omnidirectional Image Super-Resolution Method Based on Spherical Expansion</td>
      <td>Oral</td>
    </tr>
    <tr>
      <td>Revisiting Intelligent Settlement and Nutritional Estimation of Small-bowl Dishes via Deep Learning</td>
      <td>Oral</td>
    </tr>
    <tr>
      <td>Seeing in the Noisy Dark: A New Real-world Benchmark and an Efficient Method for Extreme Low-light Image Enhancement</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>Point Long-Term Locality-Aware Transformer for Point Cloud Video Understanding</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>Multi-scale Dynamic Network for Document Shadow Removal</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>Memory-Augmented Continuous-Time Neural Policy for Vision-Guided Embodied Navigation</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>PanoExtend: An Omnidirectional Image Super-Resolution Method Based on Spherical Expansion</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>A Survey on Future Physical World Generation for Autonomous Driving</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>A Survey for Point Prompt of Segment Anything Model</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>Triple-Branch Fusion Module with Spatial-Frequency Cross-Attention Mechanism for Small Object Detection</td>
      <td>Poster</td>
    </tr>
  </table>
</section>



    <!-- Organizers Section -->
    <section id="organizers">
      <h2>Organizers</h2>
      <div class="organizers-carousel">
        <div class="organizer">
            <img src="assets/imgs/avatar_organizer_1.png"/>
          <h3>Zeyu Xiao</h3>
          <p>NUS, Singapore</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_2.png"/>
          <h3>Zhuoyuan Li</h3>
          <p>USTC, China</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_3.png"/>
          <h3>Xiang Chen</h3>
          <p>NJUST, China &amp; EntroVision</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_4.png"/>
          <h3>Cong Zhang</h3>
          <p>CUHK, HKSAR</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_5.png"/>
          <h3>Hadi Amirpour</h3>
          <p>University of Klagenfurt, Austria</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_6.png"/>
          <h3>Yakun Ju</h3>
          <p>University of Leicester, UK</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_7.png"/>
          <h3>Zhiwei Xiong</h3>
          <p>USTC, China</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_8.png"/>
          <h3>Kin-Man Lam</h3>
          <p>PolyU, HKSAR</p>
        </div>
      </div>
    </section>


    <!-- Contact Section -->
    <section id="sponsors">
      <h2>Sponsors</h2>
        <p>
          This workshop is proudly sponsored by 
          <a href="https://lowlevelcv.com/" target="_blank">EntroVision</a>.
        </p>
    </section>

    <!-- Contact Section -->
    <section id="contact">
      <h2>Contact</h2>
      <p>For further inquiries, please email us at
        <script>
          const u = "zeyuxiao";
          const d = "nus.edu.sg";
          document.write(`<a href="mailto:${u}@${d}">${u}@${d}</a>`);
        </script>.
      </p>
    </section>
  </div>
</body>
</html>
